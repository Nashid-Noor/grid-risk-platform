# Deployment Approval Checklist

**Model:** Grid Outage High-Impact Risk Classifier  
**Version:** ________  
**Target environment:** ________  
**Requested by:** ________  
**Date:** ________

---

## Pre-Deployment Verification

### Data & Feature Validation

- [ ] Training data source documented and version-pinned
- [ ] Target variable definition reviewed and approved by domain expert
- [ ] Leakage audit complete — no target-derived features in input matrix
- [ ] Feature engineering logic reviewed (no look-ahead bias)
- [ ] Missing value handling strategy documented
- [ ] Train/test split strategy appropriate (random stratified for v1; time-based recommended for v2)

### Model Performance

- [ ] PR-AUC meets or exceeds agreed minimum threshold (document threshold: ________)
- [ ] ROC-AUC meets or exceeds agreed minimum threshold (document threshold: ________)
- [ ] Brier score confirms acceptable calibration (max bin gap < 0.10)
- [ ] Cross-validation results stable (std < 0.05 across folds)
- [ ] Performance compared against baseline (logistic regression) — improvement justified
- [ ] Metrics JSON artifact generated and archived

### Explainability

- [ ] SHAP summary plot reviewed — no unexpected dominant features
- [ ] Local explanations sampled for 5+ predictions — results sensible to domain expert
- [ ] No protected-class proxy features dominating importance ranking

### Bias & Fairness

- [ ] Subgroup analysis conducted across NERC regions
- [ ] Model card completed with known limitations
- [ ] Socioeconomic proxy features flagged and documented
- [ ] Intended use and out-of-scope uses clearly stated

### Infrastructure

- [ ] Model artifacts serialised and loadable on target environment
- [ ] API endpoint responds to health check
- [ ] Input validation rejects malformed requests (tested with edge cases)
- [ ] Inference latency within SLA (<200ms p99 for single record on target hardware)
- [ ] Error handling covers model-not-loaded, invalid input, and unexpected exceptions

### Monitoring & Operations

- [ ] Drift reference statistics saved from training data
- [ ] Evidently monitoring pipeline tested with synthetic drift scenario
- [ ] Retrain trigger logic reviewed and threshold agreed
- [ ] Rollback procedure documented and tested
- [ ] Alerting channels configured (log, email, on-call as appropriate)
- [ ] Monitoring report archive location confirmed

### Governance

- [ ] Model card reviewed and signed by ML lead and domain stakeholder
- [ ] Risk assessment reviewed and signed
- [ ] Monitoring plan reviewed and signed
- [ ] Data privacy review completed (no PII in training data or API responses)
- [ ] Regulatory applicability assessed (NERC CIP, state PUC, AI Act if applicable)

### Documentation

- [ ] README updated with deployment instructions
- [ ] API schema documented (OpenAPI/Swagger auto-generated by FastAPI)
- [ ] Configuration parameters documented in config.py
- [ ] All assumptions listed and reviewed

---

## Model Versioning Policy

| Rule | Detail |
|---|---|
| Version format | Semantic versioning: `MAJOR.MINOR.PATCH` |
| MAJOR bump | Target definition change, new feature set, or architecture change |
| MINOR bump | Retrained on new data with same features and target |
| PATCH bump | Bug fix in preprocessing or serving logic (no model weight change) |
| Artifact storage | Each version gets its own directory under `artifacts/` |
| Retention | Minimum 3 previous versions retained for rollback |
| Promotion | Staging → production requires this checklist signed off |

---

## Sign-Off

| Role | Name | Date | Approved |
|---|---|---|---|
| ML Engineer | ____________ | ______ | ☐ |
| Domain Expert (Grid Ops) | ____________ | ______ | ☐ |
| Engineering Manager | ____________ | ______ | ☐ |
| Compliance / Legal | ____________ | ______ | ☐ |

**Deployment approved:** ☐ Yes  ☐ No — requires remediation (detail below)

**Notes:**

---
